{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "Note: All info given is from BeautifulSoup Lab 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Imports\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Creating a soup opject from the website homepage\n",
    "res = requests.get('http://chubbygrub.com')\n",
    "print(res)\n",
    "soup = bs(res.content, 'lxml')\n",
    "soup\n",
    "\n",
    "# Step 2: Scraping homepage for each restaurant name\n",
    "restaurants_section = soup.find('div', {'class':'restaurant-buttons'})\n",
    "print(restaurants_section)\n",
    "#def function():                         #Make a scraping function of some kind in future?\n",
    "restaurants = []\n",
    "for restaurant_link in restaurants_section.find_all('a',{'class': 'btn btn-lg btn-primary'}):\n",
    "    restaurant = {}                                             # Initiates dictionary\n",
    "    restaurant['name'] = restaurant_link.text                   # Adds the name of the restaurant\n",
    "    restaurant['slug'] = restaurant_link['href'].split('/')[-1] # Adds slug(?) from href(?)\n",
    "    restaurants.append(restaurant)\n",
    "print(restaurants)\n",
    "print(restaurants_section.find_all('a',{'class': 'btn btn-lg btn-primary'}))\n",
    "\n",
    "# Step 3: Using slug to scrape each restaurant's page and creating single list of food dicts\n",
    "foods = []                   # Init empty list\n",
    "for restaurant in restaurants: # for loop for each name and slug\n",
    "    print('Scraping: ', restaurant['name']) # Print current restaurant\n",
    "    # Get request for each restaurant with dis list sitch\n",
    "    restaurant_res = requests.get('http://chubbygrub.com/restaurants/{}'.format(restaurant['slug'])) \n",
    "    restaurant_soup = bs(restaurant_res.content, 'lxml') # Init Soup\n",
    "    table = restaurant_soup.find('table', {'id': 'items'}) # NAb the food tables\n",
    "    for row in table.find('tbody').find_all('tr'):  #Looping through each row\n",
    "        cells = row.find_all('td') # Creating variable for <td/>\n",
    "        food = {} # Init Dict\n",
    "        food['restaurant'] = restaurant['name']    # These are all adding the thing...\n",
    "        food['name']       = cells[0].text         # that you'd think they're adding\n",
    "        food['category']   = cells[1].text.strip() # .strip() is removing white space\n",
    "        food['calories']   = cells[2].text\n",
    "        food['fat']        = cells[3].text\n",
    "        food['carbs']      = cells[4].text\n",
    "        foods.append(food)\n",
    "\n",
    "# Step 4: Creating a pandas from list of foods\n",
    "df = pd.DataFrame(foods)\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Step 5: Exporting to CSV\n",
    "df.to_csv('./fast_foods_date', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Lasso and Elastinet code from Kobe Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df.SHOTS_MADE\n",
    "X = df.iloc[:,1:]\n",
    "ss = StandardScaler()\n",
    "Xs = ss.fit_transform(X)\n",
    "\n",
    "ridge = RidgeCV(np.logspace(0, 5, 200), cv=10)\n",
    "ridge.fit(Xs, y)\n",
    "print(ridge.alpha_)\n",
    "\n",
    "ridge_scores = cross_val_score(Ridge(alpha = ridge.alpha_),\n",
    "                               Xs, y, cv=10)\n",
    "print(np.mean(ridge_scores))\n",
    "# This is much better because we've optomized with Ridge.\n",
    "#(Notes: Multicolinearity.  Ridge manages multicolinearity)\n",
    "\n",
    "lasso = LassoCV(n_alphas = 500, cv = 10, verbose = 0)\n",
    "lasso.fit(Xs, y)\n",
    "print(lasso.alpha_)\n",
    "\n",
    "lasso_scores = cross_val_score(Lasso(alpha=lasso.alpha_),\n",
    "                               Xs, y, cv = 10)\n",
    "print(np.mean(lasso_scores))\n",
    "# It's marginally better than Ridge.\n",
    "# Note: Lasso focuses on feature selection.\n",
    "\n",
    "lasso_model = Lasso(alpha=lasso.alpha_).fit(Xs, y)\n",
    "lasso_coefs = pd.DataFrame({'variable': X.columns,\n",
    "                            'coefs': lasso.coef_,\n",
    "                            'abs_coef': np.abs(lasso.coef_)})\n",
    "lasso_coefs.sort_values('abs_coef', \n",
    "                        inplace = True, \n",
    "                        ascending = False)\n",
    "lasso_coefs.head()\n",
    "\n",
    "enet = ElasticNetCV(l1_ratio=np.linspace(0.01, 1.0, 25),\n",
    "                    n_alphas = 100, cv = 10)\n",
    "enet.fit(Xs, y)\n",
    "print(enet.alpha_)\n",
    "print(enet.l1_ratio_)\n",
    "\n",
    "enet_scores = cross_val_score(ElasticNet(alpha=enet.alpha_,\n",
    "                                        l1_ratio=enet.l1_ratio_),\n",
    "                              Xs, y, cv = 10)\n",
    "print(np.mean(enet_scores))\n",
    "# It's doing about the same as the Lasso.\n",
    "\n",
    "#Some code for running through Cross Val Scores between two numbers of folds\n",
    "for i in range (5,11):\n",
    "    lin_reg = LinearRegression()\n",
    "    print('\\n Folds:', i)\n",
    "    #print('Cross Val Scores:',cross_val_score(lin_reg, X, y, cv=i))\n",
    "    print('Cross Val Pred R2:', \n",
    "          metrics.r2_score(y, cross_val_predict(lin_reg, X, y, cv=i)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gridsearch\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       tokenizer = tokenizer.tokenize,\n",
    "                       preprocessor = None,\n",
    "                       stop_words = 'english',\n",
    "                       min_df = 2, # This can be a float representing it's proportion of the documents\n",
    "                       max_df = None\n",
    "                       ngram_range = (min,max),\n",
    "                      max_features = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    ''\n",
    "}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "GridSearchCV()\n",
    "\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "\n",
    "Pipeline()\n",
    "make_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

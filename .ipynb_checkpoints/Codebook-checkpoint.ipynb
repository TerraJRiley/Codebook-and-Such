{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* S1 Gather Data\n",
    "    * 1.1 Webscrapping\n",
    "    * 1.2 API\n",
    "* EDA\n",
    "* Pre-processing & Feature Engineering (Use Transformers)\n",
    "    * Pd.Get_dummies/One Hot Encoding\n",
    "    * Train/test Split\n",
    "    * --->Xtrain,ytrain,Xtest,ytest\n",
    "    * interaction terms\n",
    "    * binarization (True/False-ing stuff)\n",
    "    * column selection\n",
    "    * binning\n",
    "    * standardization (scaling)\n",
    "    * CountVec\n",
    "    * fit\n",
    "    * transform\n",
    "    * --->Xtrain,ytrain,Xtest,ytest\n",
    "\n",
    "* Modelling & Evaluation\n",
    "    * Model (obtain/select evaluator)(Instantiate)\n",
    "    * Ensemble (Meta Model)\n",
    "    * score / metrics\n",
    "    * Cross Validation\n",
    "    \n",
    "* Optimization:\n",
    "    * parameter tuning (changing settings)\n",
    "    * Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['column'] >= 10 \n",
    "df = df[mask]\n",
    "# OR\n",
    "df = df[df['column'] >= 10]\n",
    "df = df.query('columm < 10')\n",
    "df.drop(df[df['column'] < 10].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['column'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved for basics & info on loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved for basics & info on iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved for basics & info on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentamentality via Textblob\n",
    "def Sentamentize(text):\n",
    "    return TextBlob(str(text)).sentiment.polarity\n",
    "count = 0\n",
    "for df in channels_obtained:\n",
    "    print(len(channels_obtained)-count, channel_names[count])\n",
    "    df['sentiment_textblob'] = df.apply(lambda x: pd.Series(Sentamentize(x),\n",
    "                                                            index=['text']),\n",
    "                                                            axis=1)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your own data just to fuck with models! :D\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "\n",
    "x,y = make_regression(n_samples=10000, n_features=20, random_state=42)\n",
    "\n",
    "X, y = make_classification(n_samples=10000,\n",
    "                           n_features=20,\n",
    "                           random_state=42\n",
    "                          )# A quick data set that just makes junk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations (Plt & Sns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of alterations to each graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all different graphs we have available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placing multiple graphs on different axes.\n",
    "# Placing two graphs on top of eachoher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating half or above a threshold heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Exporitory Data Analysis\n",
    "(what's it look like? what do i need it to look like? what're my decision and/or plan and why?) (decisions for future stuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & Feature Engineering (Use Transformers)\n",
    "  \n",
    "    * Pd.Get_dummies/One Hot Encoding\n",
    "    * Train/test Split\n",
    "    * --->Xtrain,ytrain,Xtest,ytest\n",
    "    * interaction terms\n",
    "    * binarization (True/False-ing stuff)\n",
    "    * column selection\n",
    "    * binning\n",
    "    * standardization (scaling)\n",
    "    * CountVec\n",
    "    * fit\n",
    "    * transform\n",
    "    * --->Xtrain,ytrain,Xtest,ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "1.) Tokenizing - splitting data into individual words or \"chunks\"\n",
    "2.) Regex Filtering\n",
    "3.) Lemming/Stemming\n",
    "\n",
    "\n",
    "\n",
    "- Additional Things (i.e. removing HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "document_stemmed = [p_stemmer.stem(i) for i in document_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer#, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "#tokens_lem = [lemmatizer.lemmatize(i) for i in document_tokens]\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                             min_df = 2,\n",
    "                             tokenizer = tokenizer.tokenize,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = 'english') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "document_tokens = tokenizer.tokenize(the_document.lower())\n",
    "document_tokens_lem = [lemmatizer.lemmatize(i) for i in document_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7c46c8f681ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use regular expressions to do a find-and-replace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n\u001b[0;32m----> 3\u001b[0;31m                       \" \")#,                   # The pattern to replace it with\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \u001b[0;31m#example1.get_text() )  # The text to search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(letters_only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \")#,                   # The pattern to replace it with\n",
    "                      #example1.get_text() )  # The text to search\n",
    "#print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NLP lab 5.2\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "def lem_prepro(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word) for word in tokens if not word in stop_words])\n",
    "data_preprod = [lem_prepro(text) for text in df['documents']]\n",
    "\n",
    "\n",
    "def stem_preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    p_stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([p_stemmer.stem(word) for word in tokens if not word in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From NLP Lab 5.2\n",
    "# Two NLP Pipelines One for Hashing one for TF-IDF\n",
    "model = make_pipeline(HashingVectorizer(stop_words='english', # Init Pipeline, HAshingVec & Set stopwords\n",
    "                                        non_negative = True,  # absolute value applied to feature matrix before returning\n",
    "                                        n_features = 2**16),\n",
    "                     LogisticRegression())\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_hat = model.predict(data_test['data'])\n",
    "print(accuracy_score(y_test, y_hat))\n",
    "print(\"Features:\", 2**16)\n",
    "model = make_pipeline(TfidfVectorizer(stop_words=None,\n",
    "                                      sublinear_tf = True,\n",
    "                                      max_df = 0.5,\n",
    "                                      max_features = 1000),\n",
    "                     LogisticRegression(),)\n",
    "model.fit(data_train_preprocess, y_train)\n",
    "y_hat = model.predict(data_test['data'])\n",
    "print(accuracy_score(y_test, y_hat))\n",
    "print(\"Features:\", len(model.steps[0][1].get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Standardly\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler().fit(X)\n",
    "Xs = ss.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt-code for SS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()         \n",
    "X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "X_test_scaled = ss.transform(X_test)        # Transform XTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "# We can scale ahead of time, to speed up computation time\n",
    "#ss = StandardScaler()         \n",
    "#X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "#X_test_scaled = ss.transform(X_test)        # Transform XTest\n",
    "pca = PCA()                                 # Instantiate PCA\n",
    "pca.fit(X_train)#_scaled)                     # Fit PCA to the scaled XTrain\n",
    "Z_train = pca.transform(X_train)#_scaled)\n",
    "Z_test = pca.transform(X_test)#_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA Steps:\n",
    "    1.) Calculate the correlation matrix of the feature space\n",
    "    2.) Get the eigenvectors and eigenvalues (matrix decomposition on the correlation matrix)\n",
    "    2b.) Sort by descending eigenvalue\n",
    "    3.) Choose the amount of variance we want in our feature set\n",
    "    3b.) Cumulative explained variance by eigenvalue\n",
    "    4.) Get rid of the eigenvectors I do not need (in other words, take the ones I want to keep)\n",
    "    5.) Transform the feature space accordingly\n",
    "        (By matrix multiplying the eigenvectors witht he original rows of the data)\n",
    "        \n",
    "Conceptual Understanding: What does it do to the data and how?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Model Form:\n",
    "features = ['columns_etc']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "model = Model(**params).fit(Xtrain,ytrain)\n",
    "model.score(Xtrain,ytrain)\n",
    "model.score(Xtest,ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# t/ts and StandardScaler here-ish?\n",
    "lin_reg = LinearRegression()\n",
    "features = ['columns_etc']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "model = lin_reg.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# Logistic Regression\n",
    "# K-Nearest Neighbors\n",
    "# Nieve Bayes\n",
    "    # BinomialNB\n",
    "    # MultinomialNB\n",
    "    # GaussianNB\n",
    "# DEcisionTree Classifier\n",
    "    # Random Forest\n",
    "    # Extra Trees\n",
    "# Boosters\n",
    "    # Ada Boost\n",
    "    # Graident Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Regressions:\n",
    "scaled_df = pd.DataFrame(StandardScaler().fit_transform(df),\n",
    "                         columns = df.columns)\n",
    "scaled_df.head()\n",
    "\n",
    "X = scaled_df.drop(columns = ['target'])\n",
    "y = scaled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "lin_reg    = LinearRegression().fit(X_train, y_train)\n",
    "knn_reg    = KNeighborsRegressor().fit(X_train, y_train)\n",
    "cart_reg   = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "bag_reg    = BaggingRegressor().fit(X_train, y_train)\n",
    "randof_reg = RandomForestRegressor().fit(X_train, y_train)\n",
    "ada_reg    = AdaBoostRegressor().fit(X_train, y_train)\n",
    "SV_reg     = SVR().fit(X_train, y_train)\n",
    "models     = [\n",
    "    lin_reg,\n",
    "    knn_reg,\n",
    "    cart_reg,\n",
    "    bag_reg,\n",
    "    randof_reg,\n",
    "    ada_reg,\n",
    "    SV_reg    \n",
    "]\n",
    "def rmse_score(model, \n",
    "               X_train = X_train,\n",
    "               X_test = X_test,\n",
    "               y_train = y_train,\n",
    "               y_test = y_test):\n",
    "    train_score = mean_squared_error(y_true = y_train,\n",
    "                                     y_pred = model.predict(X_train)) ** 0.5\n",
    "    test_score = mean_squared_error(y_true = y_test,\n",
    "                                    y_pred = model.predict(X_test)) ** 0.5\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return\n",
    "for model in models:\n",
    "    rmse_score(model)\n",
    "\n",
    "# Classifications:\n",
    "X = scaled_df.drop(columns = ['e401k', 'p401k'])\n",
    "y = [1 if scaled_df['e401k'][i] > 0 else 0 for i in range(scaled_df.shape[0])]\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "log_class    = LogisticRegression().fit(Xr_train, yr_train)\n",
    "knn_class    = KNeighborsClassifier().fit(Xr_train, yr_train)\n",
    "cart_class   = DecisionTreeClassifier().fit(Xr_train, yr_train)\n",
    "bag_class    = BaggingClassifier().fit(Xr_train, yr_train)\n",
    "randof_class = RandomForestClassifier().fit(Xr_train, yr_train)\n",
    "ada_class    = AdaBoostClassifier().fit(Xr_train, yr_train)\n",
    "SV_class     = SVC().fit(Xr_train, yr_train)\n",
    "models     = [\n",
    "    log_class,\n",
    "    knn_class,\n",
    "    cart_class,\n",
    "    bag_class,\n",
    "    randof_class,\n",
    "    ada_class,\n",
    "    SV_class    \n",
    "]\n",
    "def f1_score(model, \n",
    "               X_train = Xr_train,\n",
    "               X_test = Xr_test,\n",
    "               y_train = yr_train,\n",
    "               y_test = yr_test):\n",
    "    train_score = f1_score(y_train,\n",
    "                           model.predict(X_train))\n",
    "    test_score  = f1_score(y_test,\n",
    "                           model.predict(X_test))\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Lasso and Elastinet code from Kobe Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "y = df.SHOTS_MADE\n",
    "X = df.iloc[:,1:]\n",
    "ss = StandardScaler()\n",
    "Xs = ss.fit_transform(X)\n",
    "\n",
    "ridge = RidgeCV(np.logspace(0, 5, 200), cv=10)\n",
    "ridge.fit(Xs, y)\n",
    "print(ridge.alpha_)\n",
    "\n",
    "ridge_scores = cross_val_score(Ridge(alpha = ridge.alpha_),\n",
    "                               Xs, y, cv=10)\n",
    "print(np.mean(ridge_scores))\n",
    "# This is much better because we've optomized with Ridge.\n",
    "#(Notes: Multicolinearity.  Ridge manages multicolinearity)\n",
    "\n",
    "lasso = LassoCV(n_alphas = 500, cv = 10, verbose = 0)\n",
    "lasso.fit(Xs, y)\n",
    "print(lasso.alpha_)\n",
    "\n",
    "lasso_scores = cross_val_score(Lasso(alpha=lasso.alpha_),\n",
    "                               Xs, y, cv = 10)\n",
    "print(np.mean(lasso_scores))\n",
    "# It's marginally better than Ridge.\n",
    "# Note: Lasso focuses on feature selection.\n",
    "\n",
    "lasso_model = Lasso(alpha=lasso.alpha_).fit(Xs, y)\n",
    "lasso_coefs = pd.DataFrame({'variable': X.columns,\n",
    "                            'coefs': lasso.coef_,\n",
    "                            'abs_coef': np.abs(lasso.coef_)})\n",
    "lasso_coefs.sort_values('abs_coef', \n",
    "                        inplace = True, \n",
    "                        ascending = False)\n",
    "lasso_coefs.head()\n",
    "\n",
    "enet = ElasticNetCV(l1_ratio=np.linspace(0.01, 1.0, 25),\n",
    "                    n_alphas = 100, cv = 10)\n",
    "enet.fit(Xs, y)\n",
    "print(enet.alpha_)\n",
    "print(enet.l1_ratio_)\n",
    "\n",
    "enet_scores = cross_val_score(ElasticNet(alpha=enet.alpha_,\n",
    "                                        l1_ratio=enet.l1_ratio_),\n",
    "                              Xs, y, cv = 10)\n",
    "print(np.mean(enet_scores))\n",
    "# It's doing about the same as the Lasso.\n",
    "\n",
    "#Some code for running through Cross Val Scores between two numbers of folds\n",
    "for i in range (5,11):\n",
    "    lin_reg = LinearRegression()\n",
    "    print('\\n Folds:', i)\n",
    "    #print('Cross Val Scores:',cross_val_score(lin_reg, X, y, cv=i))\n",
    "    print('Cross Val Pred R2:', metrics.r2_score(y, cross_val_predict(lin_reg, X, y, cv=i)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #Usually for Bounded or unbounded scale, don't remember which\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical # For the MultiClass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Train/Test Split & Scale:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()                                # Note: Start here.  Grow & add after 1st model\n",
    "model.add(Dense(neurons_for_this_layer,             # Start == to # of features, but not over 100\n",
    "          activation = 'relu',                      # Default.  Rarely need to change\n",
    "          input_dim=X_train.shape[1]))   # Keeps consistent without hasstle, not needed post-1st layer\n",
    "#model.add(Dense(neurons_for_this_layer, activation = 'relu')) # Here, have another layer\n",
    "model.add(Dense(1, activation=None))     # Output layer.  Does not change from this for Regression\n",
    "model.compile(loss='mean_squared_error',            # Compile tells Keras we're done adding layers\n",
    "             optimizer='adam')                      # Default.  Rarely need to change\n",
    "results = model.fit(X_train_scaled, y_train, \n",
    "                    epochs= 10,                               # Default is 1, but start with about 10\n",
    "                    batch_size = 32,                          # Skip if data is small. Should be powers of 2\n",
    "                    validation_data = (X_test_scaled,y_test)) # Note: Val Loss is our Test loss\n",
    "                    #verbose=0,                               # This will silence printing output\n",
    "model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16* 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure to stratify the train/test when you do that, for this model type\n",
    "model = Sequential()                                # Note: Start here.  Grow & add after 1st model\n",
    "model.add(Dense(neurons_for_this_layer,             # Start == to # of features, but not over 100\n",
    "          activation = 'relu',                      # Default.  Rarely need to change\n",
    "          input_dim=X_train.shape[1]))              # Keeps consistent without hasstle, not needed post-1st layer\n",
    "#model.add(Dense(neurons_for_this_layer, activation = 'relu')) # Here, have another layer\n",
    "model.add(Dense(1, activation='sigmoid')) # Output Layer.  Not to be changed from this for Binary Classification\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',           # Loss function for Binary Classification\n",
    "#             metrics=['accuracy']                  # Optional metric\n",
    "             )\n",
    "\n",
    "results = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=10,                      # Default is 1, but start with about 10\n",
    "                    #batch_size = 32,               # Unsure if useable in this model, but whatever\n",
    "                    #verbose=0,                     # This will silence printing output\n",
    "                    validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train) # Note: Multi-classification specific prep\n",
    "y_test  = to_categorical(y_test)  # Note: This one-hot encodes the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual model itself\n",
    "model = Sequential()\n",
    "model.add(Dense(4,                                # Start == to # of features, but not over 100\n",
    "               activation='relu',                 # Default.  Rarely Change\n",
    "               input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dense(num_of_classes_we_are_predicting, # This is the Output Layer\n",
    "                activation='softmax'))            # Activation is specific to the output layer\n",
    "model.compile(optimizer='adam',                   # Default.  Rarely change.\n",
    "#             metrics=['accuracy'],               # This is optional, if we wish to track it\n",
    "             loss = 'categorical_crossentropy')   # Default for this type?\n",
    "results = model.fit(X_train_scaled, y_train,\n",
    "                    validation_data = (X_test_scaled, y_test),\n",
    "                    #verbose=0,                   # This will silence printing output\n",
    "                    epochs = 10)                  # One of the first things to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations for classification Models\n",
    "# Plots to view Loss/Accuracy over epoche's\n",
    "plt.plot(results.history['val_loss'], label='Test Loss')\n",
    "plt.plot(results.history['loss'], label='Train Loss')\n",
    "plt.legend();\n",
    "plt.plot(results.history['val_acc'], label='Test Accuracy')\n",
    "plt.plot(results.history['acc'], label='Train Accuracy')\n",
    "plt.legend();\n",
    "\n",
    "#Alternative\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15,4))\n",
    "\n",
    "ax[0].plot(results.history['loss'])\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[0].set_xlabel(\"epochs\")\n",
    "\n",
    "ax[1].plot(results.history['acc'])\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[1].set_xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "kmean = KMeans(n_clusters=3)\n",
    "kmean.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF out of the Centroids\n",
    "centroids_df = pd.DataFrame(kmean.cluster_centers_, columns= [\"Pu239\",'Pu240'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space Reserved for Function to plot in scatter form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score for Clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silho_score = silhouette_score(Xs_df, kms.labels_)\n",
    "silho_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "def ALL_METRICS(y_real, y_hat, p = X.shape[1]):\n",
    "    print('Mean Squared Error      :',metrics.mean_squared_error(y_real, y_hat))\n",
    "    print('Root Mean Squared Error :',np.sqrt(metrics.mean_squared_error(y_real, y_hat)))\n",
    "    print('Mean Squared Log Error  :',metrics.mean_squared_log_error(y_real, y_hat))\n",
    "    print('Median Absolute Error   :',metrics.median_absolute_error(y_real,y_hat))\n",
    "    print('R Squared               :',metrics.r2_score(y_real, y_hat))\n",
    "    print('Adjusted R Squared      :',r2_adj(y_real, y_hat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crontab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "time.time() # The time now, measured in seconds\n",
    "t0 = time.time()\n",
    "\n",
    "# Code goes here.\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big O Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgres://'+  # Something\n",
    "                       'u6jk064rvr8dao:'+ #Username\n",
    "                       'p0f453685110bbc3b5c17c10350750f46b60666c388a78134b4c5dc792da985f7'+ # Password\n",
    "                       '@ec2-34-202-213-35.compute-1.amazonaws.com:5432/'+ # The connection to it\n",
    "                       'dc5rooirs71hh0')  # ssssomething else\n",
    "   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

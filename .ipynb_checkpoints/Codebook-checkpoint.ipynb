{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* S1 Gather Data\n",
    "    * 1.1 Webscrapping\n",
    "    * 1.2 API\n",
    "* EDA\n",
    "* Pre-processing & Feature Engineering (Use Transformers)\n",
    "    * Pd.Get_dummies/One Hot Encoding\n",
    "    * Train/test Split\n",
    "    * --->Xtrain,ytrain,Xtest,ytest\n",
    "    * interaction terms\n",
    "    * binarization (True/False-ing stuff)\n",
    "    * column selection\n",
    "    * binning\n",
    "    * standardization (scaling)\n",
    "    * CountVec\n",
    "    * fit\n",
    "    * transform\n",
    "    * --->Xtrain,ytrain,Xtest,ytest\n",
    "\n",
    "* Modelling & Evaluation\n",
    "    * Model (obtain/select evaluator)(Instantiate)\n",
    "    * Ensemble (Meta Model)\n",
    "    * score / metrics\n",
    "    * Cross Validation\n",
    "    \n",
    "* Optimization:\n",
    "    * parameter tuning (changing settings)\n",
    "    * Gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "\n",
    "df.dtypes                           # Data Types of cols\n",
    "df.column.unique()                  # Unique Values in col\n",
    "df.describe().T                     # Summary Stats\n",
    "df.info                             # Info\n",
    "df['column'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential Alt way to obtain Cirrelations\n",
    "column_1 = df.column_1.values\n",
    "column_2 = df.column_2.values\n",
    "print(np.corrcoef(column_1, column_2))\n",
    "\n",
    "# Needs to be tested:\n",
    "target_column = df.target.values\n",
    "other_columns = df.columns\n",
    "for column in other_columns:\n",
    "    print(np.corrcoef(target_column,column)[0][1], column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['column'] >= 10 \n",
    "df = df[mask]\n",
    "# OR\n",
    "df = df[df['column'] >= 10]\n",
    "df = df.query('columm < 10')\n",
    "df.drop(df[df['column'] < 10].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved for basics & info on loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved for basics & info on iloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Columns\n",
    "new_names = ['list of new names in exact order']\n",
    "new_names = {'old_name': 'new_name'}\n",
    "df.columns = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses .map() to replace '?'s with np.nan and the ints into floats\n",
    "df.column = df.column.map(lambda x: np.nan if x == '?' else float(x))\n",
    "\n",
    "# Uses.map() to turn commas into periods & convert into floats:\n",
    "df.column = df.column.map(lambda x: float(x.replace(',','.')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Pandas Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentamentality via Textblob\n",
    "def Sentamentize(text):\n",
    "    return TextBlob(str(text)).sentiment.polarity\n",
    "count = 0\n",
    "for df in channels_obtained:                     # This code may be broken\n",
    "    print(len(channels_obtained)-count, channel_names[count])\n",
    "    df['sentiment_textblob'] = df.apply(lambda x: pd.Series(Sentamentize(x),\n",
    "                                                            index=['text']),\n",
    "                                                            axis=1)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates data such that we can focus on models :)\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "x, y =     make_regression(n_samples=10000, n_features=20)\n",
    "X, y = make_classification(n_samples=10000, n_features=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations (Plt & Sns)\n",
    "\n",
    "Remember the six \"lessons\" of visualizing data.\n",
    " - Lesson 1: Understand the context. (Who, What, How)\n",
    " - Lesson 2: Choose an appropriate visual display. (What type of graph?)\n",
    " - Lesson 3: Eliminate clutter. (What can I delete or make more subtle?)\n",
    " - Lesson 4: Focus attention where you want it. (How do I emphasize important things?)\n",
    " - Lesson 5: Think like a designer. (How do I organize my visualization?)\n",
    " - Lesson 6: Tell a story! (How do I communicate my visualization?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph Alterations\n",
    "# Colors: plt.colors.cnames\n",
    "\n",
    "\n",
    "# For layering on top:\n",
    "plt.hist(df['column_1'], color='g')\n",
    "plt.hist(df['column_2'], color='b')\n",
    "\n",
    "# For layering next to:\n",
    "fig, ax = plt.subplots(2, \n",
    "                    sharex = True,  # Scale x together\n",
    "                    sharey = True,    \n",
    "                   figsize = (1,1)) # Size\n",
    "ax[0].hist(df['column_1'],          # \n",
    "           bins = 10,               # Bins\n",
    "           color='b',               # Color\n",
    "           alpha = 0.5,             # Transparancy\n",
    "           label = 'col_1')         # For .legend()\n",
    "ax[1].hist(df['column_2'])          # \n",
    "\n",
    "# Labels for Words\n",
    "plt.xlabel(\"measure\",               # Alt: .ylabel\n",
    "           position = (0,0),        # (x-pos, y-pos)?\n",
    "           ha = 'left',             # Horizontal\n",
    "           color = 'grey') # (x,y)\n",
    "\n",
    "plt.legend()\n",
    "plt.title()                         # Title ~= Subtitle\n",
    "plt.suptitle(\"subtitle\\n.\",         # Subtitle\n",
    "             position = (0,1),      # Position\n",
    "             ha = 'left',           # Horizontal\n",
    "             fontsize=16,           # Size, Font\n",
    "             va = 'top');           # Vertical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aliceblue': '#F0F8FF',\n",
       " 'antiquewhite': '#FAEBD7',\n",
       " 'aqua': '#00FFFF',\n",
       " 'aquamarine': '#7FFFD4',\n",
       " 'azure': '#F0FFFF',\n",
       " 'beige': '#F5F5DC',\n",
       " 'bisque': '#FFE4C4',\n",
       " 'black': '#000000',\n",
       " 'blanchedalmond': '#FFEBCD',\n",
       " 'blue': '#0000FF',\n",
       " 'blueviolet': '#8A2BE2',\n",
       " 'brown': '#A52A2A',\n",
       " 'burlywood': '#DEB887',\n",
       " 'cadetblue': '#5F9EA0',\n",
       " 'chartreuse': '#7FFF00',\n",
       " 'chocolate': '#D2691E',\n",
       " 'coral': '#FF7F50',\n",
       " 'cornflowerblue': '#6495ED',\n",
       " 'cornsilk': '#FFF8DC',\n",
       " 'crimson': '#DC143C',\n",
       " 'cyan': '#00FFFF',\n",
       " 'darkblue': '#00008B',\n",
       " 'darkcyan': '#008B8B',\n",
       " 'darkgoldenrod': '#B8860B',\n",
       " 'darkgray': '#A9A9A9',\n",
       " 'darkgreen': '#006400',\n",
       " 'darkgrey': '#A9A9A9',\n",
       " 'darkkhaki': '#BDB76B',\n",
       " 'darkmagenta': '#8B008B',\n",
       " 'darkolivegreen': '#556B2F',\n",
       " 'darkorange': '#FF8C00',\n",
       " 'darkorchid': '#9932CC',\n",
       " 'darkred': '#8B0000',\n",
       " 'darksalmon': '#E9967A',\n",
       " 'darkseagreen': '#8FBC8F',\n",
       " 'darkslateblue': '#483D8B',\n",
       " 'darkslategray': '#2F4F4F',\n",
       " 'darkslategrey': '#2F4F4F',\n",
       " 'darkturquoise': '#00CED1',\n",
       " 'darkviolet': '#9400D3',\n",
       " 'deeppink': '#FF1493',\n",
       " 'deepskyblue': '#00BFFF',\n",
       " 'dimgray': '#696969',\n",
       " 'dimgrey': '#696969',\n",
       " 'dodgerblue': '#1E90FF',\n",
       " 'firebrick': '#B22222',\n",
       " 'floralwhite': '#FFFAF0',\n",
       " 'forestgreen': '#228B22',\n",
       " 'fuchsia': '#FF00FF',\n",
       " 'gainsboro': '#DCDCDC',\n",
       " 'ghostwhite': '#F8F8FF',\n",
       " 'gold': '#FFD700',\n",
       " 'goldenrod': '#DAA520',\n",
       " 'gray': '#808080',\n",
       " 'green': '#008000',\n",
       " 'greenyellow': '#ADFF2F',\n",
       " 'grey': '#808080',\n",
       " 'honeydew': '#F0FFF0',\n",
       " 'hotpink': '#FF69B4',\n",
       " 'indianred': '#CD5C5C',\n",
       " 'indigo': '#4B0082',\n",
       " 'ivory': '#FFFFF0',\n",
       " 'khaki': '#F0E68C',\n",
       " 'lavender': '#E6E6FA',\n",
       " 'lavenderblush': '#FFF0F5',\n",
       " 'lawngreen': '#7CFC00',\n",
       " 'lemonchiffon': '#FFFACD',\n",
       " 'lightblue': '#ADD8E6',\n",
       " 'lightcoral': '#F08080',\n",
       " 'lightcyan': '#E0FFFF',\n",
       " 'lightgoldenrodyellow': '#FAFAD2',\n",
       " 'lightgray': '#D3D3D3',\n",
       " 'lightgreen': '#90EE90',\n",
       " 'lightgrey': '#D3D3D3',\n",
       " 'lightpink': '#FFB6C1',\n",
       " 'lightsalmon': '#FFA07A',\n",
       " 'lightseagreen': '#20B2AA',\n",
       " 'lightskyblue': '#87CEFA',\n",
       " 'lightslategray': '#778899',\n",
       " 'lightslategrey': '#778899',\n",
       " 'lightsteelblue': '#B0C4DE',\n",
       " 'lightyellow': '#FFFFE0',\n",
       " 'lime': '#00FF00',\n",
       " 'limegreen': '#32CD32',\n",
       " 'linen': '#FAF0E6',\n",
       " 'magenta': '#FF00FF',\n",
       " 'maroon': '#800000',\n",
       " 'mediumaquamarine': '#66CDAA',\n",
       " 'mediumblue': '#0000CD',\n",
       " 'mediumorchid': '#BA55D3',\n",
       " 'mediumpurple': '#9370DB',\n",
       " 'mediumseagreen': '#3CB371',\n",
       " 'mediumslateblue': '#7B68EE',\n",
       " 'mediumspringgreen': '#00FA9A',\n",
       " 'mediumturquoise': '#48D1CC',\n",
       " 'mediumvioletred': '#C71585',\n",
       " 'midnightblue': '#191970',\n",
       " 'mintcream': '#F5FFFA',\n",
       " 'mistyrose': '#FFE4E1',\n",
       " 'moccasin': '#FFE4B5',\n",
       " 'navajowhite': '#FFDEAD',\n",
       " 'navy': '#000080',\n",
       " 'oldlace': '#FDF5E6',\n",
       " 'olive': '#808000',\n",
       " 'olivedrab': '#6B8E23',\n",
       " 'orange': '#FFA500',\n",
       " 'orangered': '#FF4500',\n",
       " 'orchid': '#DA70D6',\n",
       " 'palegoldenrod': '#EEE8AA',\n",
       " 'palegreen': '#98FB98',\n",
       " 'paleturquoise': '#AFEEEE',\n",
       " 'palevioletred': '#DB7093',\n",
       " 'papayawhip': '#FFEFD5',\n",
       " 'peachpuff': '#FFDAB9',\n",
       " 'peru': '#CD853F',\n",
       " 'pink': '#FFC0CB',\n",
       " 'plum': '#DDA0DD',\n",
       " 'powderblue': '#B0E0E6',\n",
       " 'purple': '#800080',\n",
       " 'rebeccapurple': '#663399',\n",
       " 'red': '#FF0000',\n",
       " 'rosybrown': '#BC8F8F',\n",
       " 'royalblue': '#4169E1',\n",
       " 'saddlebrown': '#8B4513',\n",
       " 'salmon': '#FA8072',\n",
       " 'sandybrown': '#F4A460',\n",
       " 'seagreen': '#2E8B57',\n",
       " 'seashell': '#FFF5EE',\n",
       " 'sienna': '#A0522D',\n",
       " 'silver': '#C0C0C0',\n",
       " 'skyblue': '#87CEEB',\n",
       " 'slateblue': '#6A5ACD',\n",
       " 'slategray': '#708090',\n",
       " 'slategrey': '#708090',\n",
       " 'snow': '#FFFAFA',\n",
       " 'springgreen': '#00FF7F',\n",
       " 'steelblue': '#4682B4',\n",
       " 'tan': '#D2B48C',\n",
       " 'teal': '#008080',\n",
       " 'thistle': '#D8BFD8',\n",
       " 'tomato': '#FF6347',\n",
       " 'turquoise': '#40E0D0',\n",
       " 'violet': '#EE82EE',\n",
       " 'wheat': '#F5DEB3',\n",
       " 'white': '#FFFFFF',\n",
       " 'whitesmoke': '#F5F5F5',\n",
       " 'yellow': '#FFFF00',\n",
       " 'yellowgreen': '#9ACD32'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of all different graphs we have available\n",
    "\n",
    "# WTF ARE THESE THINGS!?!?\n",
    "matplotlib.Button\n",
    "plt.Figure\n",
    "plt.Polygon\n",
    "plt.axis\n",
    "plt.draw\n",
    "plt.draw_if_interactive\n",
    "plt.eventplot\n",
    "plt.matplotlib.widgets\n",
    "plt.plot_date\n",
    "plt.specgram\n",
    "plt.spectral # Spectral Color Map!? :D\n",
    "plt.stackplot\n",
    "plt.subplot\n",
    "plt.table\n",
    "plt.waitforbuttonpress\n",
    "plt.xcorr\n",
    "\n",
    "# Eaily usable plots\n",
    "plt.boxplot\n",
    "plt.hist\n",
    "plt.hist2d\n",
    "plt.plot\n",
    "plt.scatter\n",
    "plt.violinplot\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pearl/anaconda3/envs/dsi/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/pearl/anaconda3/envs/dsi/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot()\n",
    "sns.pairplot()\n",
    "sns.puppyplot()\n",
    "sns.interactplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(df.corr(), annot=True)\n",
    "\n",
    "# Isolating half or above a threshold heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & Feature Engineering (Use Transformers)\n",
    "  \n",
    "    * Pd.Get_dummies/One Hot Encoding\n",
    "    * Train/test Split\n",
    "    * --->Xtrain,ytrain,Xtest,ytest\n",
    "    * interaction terms\n",
    "    * binarization (True/False-ing stuff)\n",
    "    * column selection\n",
    "    * binning\n",
    "    * standardization (scaling)\n",
    "    * CountVec\n",
    "    * fit\n",
    "    * transform\n",
    "    * --->Xtrain,ytrain,Xtest,ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "1.) Tokenizing - splitting data into individual words or \"chunks\"\n",
    "2.) Regex Filtering\n",
    "3.) Lemming/Stemming\n",
    "\n",
    "\n",
    "\n",
    "- Additional Things (i.e. removing HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "document_stemmed = [p_stemmer.stem(i) for i in document_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer#, TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word) for word in tokens if len(word) > 1 and not word in stop_words])\n",
    "cvec = CountVectorizer(analyzer = \"word\",\n",
    "                       tokenizer = tokenizer.tokenize,\n",
    "                       preprocessor = preprocess,\n",
    "                       stop_words = 'english',\n",
    "                       min_df = 2, # This can be a float representing it's proportion of the documents\n",
    "                       max_df = None\n",
    "                       #ngram_range = (min,max),\n",
    "                       max_features = None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "document_tokens = tokenizer.tokenize(the_document.lower())\n",
    "document_tokens_lem = [lemmatizer.lemmatize(i) for i in document_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sub() missing 1 required positional argument: 'string'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7c46c8f681ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Use regular expressions to do a find-and-replace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n\u001b[0;32m----> 3\u001b[0;31m                       \" \")#,                   # The pattern to replace it with\n\u001b[0m\u001b[1;32m      4\u001b[0m                       \u001b[0;31m#example1.get_text() )  # The text to search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(letters_only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sub() missing 1 required positional argument: 'string'"
     ]
    }
   ],
   "source": [
    "# Use regular expressions to do a find-and-replace\n",
    "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
    "                      \" \")#,                   # The pattern to replace it with\n",
    "                      #example1.get_text() )  # The text to search\n",
    "#print(letters_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from NLP lab 5.2\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "def lem_prepro(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([lemmer.lemmatize(word) for word in tokens if not word in stop_words])\n",
    "data_preprod = [lem_prepro(text) for text in df['documents']]\n",
    "\n",
    "\n",
    "def stem_preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z]',' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    p_stemmer = PorterStemmer()\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    return \" \".join([p_stemmer.stem(word) for word in tokens if not word in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From NLP Lab 5.2\n",
    "# Two NLP Pipelines One for Hashing one for TF-IDF\n",
    "model = make_pipeline(HashingVectorizer(stop_words='english', # Init Pipeline, HAshingVec & Set stopwords\n",
    "                                        non_negative = True,  # absolute value applied to feature matrix before returning\n",
    "                                        n_features = 2**16),\n",
    "                     LogisticRegression())\n",
    "model.fit(data_train['data'], y_train)\n",
    "y_hat = model.predict(data_test['data'])\n",
    "print(accuracy_score(y_test, y_hat))\n",
    "print(\"Features:\", 2**16)\n",
    "model = make_pipeline(TfidfVectorizer(stop_words=None,\n",
    "                                      sublinear_tf = True,\n",
    "                                      max_df = 0.5,\n",
    "                                      max_features = 1000),\n",
    "                     LogisticRegression(),)\n",
    "model.fit(data_train_preprocess, y_train)\n",
    "y_hat = model.predict(data_test['data'])\n",
    "print(accuracy_score(y_test, y_hat))\n",
    "print(\"Features:\", len(model.steps[0][1].get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Standardly\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler().fit(X)\n",
    "Xs = ss.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt-code for SS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()         \n",
    "X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "X_test_scaled = ss.transform(X_test)        # Transform XTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split & Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify = y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.preprocessing import StandardScaler\n",
    "# We can scale ahead of time, to speed up computation time\n",
    "#ss = StandardScaler()         \n",
    "#X_train_scaled = ss.fit_transform(X_train)  # Fit & Transform XTrain\n",
    "#X_test_scaled = ss.transform(X_test)        # Transform XTest\n",
    "pca = PCA()                                 # Instantiate PCA\n",
    "pca.fit(X_train)#_scaled)                     # Fit PCA to the scaled XTrain\n",
    "Z_train = pca.transform(X_train)#_scaled)\n",
    "Z_test = pca.transform(X_test)#_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA Steps:\n",
    "    1.) Calculate the correlation matrix of the feature space\n",
    "    2.) Get the eigenvectors and eigenvalues (matrix decomposition on the correlation matrix)\n",
    "    2b.) Sort by descending eigenvalue\n",
    "    3.) Choose the amount of variance we want in our feature set\n",
    "    3b.) Cumulative explained variance by eigenvalue\n",
    "    4.) Get rid of the eigenvectors I do not need (in other words, take the ones I want to keep)\n",
    "    5.) Transform the feature space accordingly\n",
    "        (By matrix multiplying the eigenvectors witht he original rows of the data)\n",
    "        \n",
    "Conceptual Understanding: What does it do to the data and how?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Model Form:\n",
    "features = ['columns_etc']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "model = Model(**params).fit(Xtrain,ytrain)\n",
    "model.score(Xtrain,ytrain)\n",
    "model.score(Xtest,ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# t/ts and StandardScaler here-ish?\n",
    "lin_reg = LinearRegression()\n",
    "features = ['columns_etc']\n",
    "X = df[features]\n",
    "y = df['target']\n",
    "\n",
    "model = lin_reg.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# Logistic Regression\n",
    "# K-Nearest Neighbors\n",
    "# Nieve Bayes\n",
    "    # BinomialNB\n",
    "    # MultinomialNB\n",
    "    # GaussianNB\n",
    "# DEcisionTree Classifier\n",
    "    # Random Forest\n",
    "    # Extra Trees\n",
    "# Boosters\n",
    "    # Ada Boost\n",
    "    # Graident Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Regressions:\n",
    "scaled_df = pd.DataFrame(StandardScaler().fit_transform(df),\n",
    "                         columns = df.columns)\n",
    "scaled_df.head()\n",
    "\n",
    "X = scaled_df.drop(columns = ['target'])\n",
    "y = scaled_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "lin_reg    = LinearRegression().fit(X_train, y_train)\n",
    "knn_reg    = KNeighborsRegressor().fit(X_train, y_train)\n",
    "cart_reg   = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "bag_reg    = BaggingRegressor().fit(X_train, y_train)\n",
    "randof_reg = RandomForestRegressor().fit(X_train, y_train)\n",
    "ada_reg    = AdaBoostRegressor().fit(X_train, y_train)\n",
    "SV_reg     = SVR().fit(X_train, y_train)\n",
    "models     = [\n",
    "    lin_reg,\n",
    "    knn_reg,\n",
    "    cart_reg,\n",
    "    bag_reg,\n",
    "    randof_reg,\n",
    "    ada_reg,\n",
    "    SV_reg    \n",
    "]\n",
    "def rmse_score(model, \n",
    "               X_train = X_train,\n",
    "               X_test = X_test,\n",
    "               y_train = y_train,\n",
    "               y_test = y_test):\n",
    "    train_score = mean_squared_error(y_true = y_train,\n",
    "                                     y_pred = model.predict(X_train)) ** 0.5\n",
    "    test_score = mean_squared_error(y_true = y_test,\n",
    "                                    y_pred = model.predict(X_test)) ** 0.5\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return\n",
    "for model in models:\n",
    "    rmse_score(model)\n",
    "\n",
    "# Classifications:\n",
    "X = scaled_df.drop(columns = ['e401k', 'p401k'])\n",
    "y = [1 if scaled_df['e401k'][i] > 0 else 0 for i in range(scaled_df.shape[0])]\n",
    "Xr_train, Xr_test, yr_train, yr_test = train_test_split(X, y,\n",
    "                                                    test_size = 0.2)\n",
    "log_class    = LogisticRegression().fit(Xr_train, yr_train)\n",
    "knn_class    = KNeighborsClassifier().fit(Xr_train, yr_train)\n",
    "cart_class   = DecisionTreeClassifier().fit(Xr_train, yr_train)\n",
    "bag_class    = BaggingClassifier().fit(Xr_train, yr_train)\n",
    "randof_class = RandomForestClassifier().fit(Xr_train, yr_train)\n",
    "ada_class    = AdaBoostClassifier().fit(Xr_train, yr_train)\n",
    "SV_class     = SVC().fit(Xr_train, yr_train)\n",
    "models     = [\n",
    "    log_class,\n",
    "    knn_class,\n",
    "    cart_class,\n",
    "    bag_class,\n",
    "    randof_class,\n",
    "    ada_class,\n",
    "    SV_class    \n",
    "]\n",
    "def f1_score(model, \n",
    "               X_train = Xr_train,\n",
    "               X_test = Xr_test,\n",
    "               y_train = yr_train,\n",
    "               y_test = yr_test):\n",
    "    train_score = f1_score(y_train,\n",
    "                           model.predict(X_train))\n",
    "    test_score  = f1_score(y_test,\n",
    "                           model.predict(X_test))\n",
    "    print(str(model)[0:20])\n",
    "    print(\"Train:\" + str(train_score))\n",
    "    print(\"Test :\" + str(test_score))\n",
    "    print('')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #Usually for Bounded or unbounded scale, don't remember which\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical # For the MultiClass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Train/Test Split & Scale:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "ss = StandardScaler()\n",
    "X_train_scaled = ss.fit_transform(X_train)\n",
    "X_test_scaled = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()                                # Note: Start here.  Grow & add after 1st model\n",
    "model.add(Dense(neurons_for_this_layer,             # Start == to # of features, but not over 100\n",
    "          activation = 'relu',                      # Default.  Rarely need to change\n",
    "          input_dim=X_train.shape[1]))   # Keeps consistent without hasstle, not needed post-1st layer\n",
    "#model.add(Dense(neurons_for_this_layer, activation = 'relu')) # Here, have another layer\n",
    "model.add(Dense(1, activation=None))     # Output layer.  Does not change from this for Regression\n",
    "model.compile(loss='mean_squared_error',            # Compile tells Keras we're done adding layers\n",
    "             optimizer='adam')                      # Default.  Rarely need to change\n",
    "results = model.fit(X_train_scaled, y_train, \n",
    "                    epochs= 10,                               # Default is 1, but start with about 10\n",
    "                    batch_size = 32,                          # Skip if data is small. Should be powers of 2\n",
    "                    validation_data = (X_test_scaled,y_test)) # Note: Val Loss is our Test loss\n",
    "                    #verbose=0,                               # This will silence printing output\n",
    "model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16* 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure to stratify the train/test when you do that, for this model type\n",
    "model = Sequential()                                # Note: Start here.  Grow & add after 1st model\n",
    "model.add(Dense(neurons_for_this_layer,             # Start == to # of features, but not over 100\n",
    "          activation = 'relu',                      # Default.  Rarely need to change\n",
    "          input_dim=X_train.shape[1]))              # Keeps consistent without hasstle, not needed post-1st layer\n",
    "#model.add(Dense(neurons_for_this_layer, activation = 'relu')) # Here, have another layer\n",
    "model.add(Dense(1, activation='sigmoid')) # Output Layer.  Not to be changed from this for Binary Classification\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',           # Loss function for Binary Classification\n",
    "#             metrics=['accuracy']                  # Optional metric\n",
    "             )\n",
    "\n",
    "results = model.fit(X_train_scaled, y_train,\n",
    "                    epochs=10,                      # Default is 1, but start with about 10\n",
    "                    #batch_size = 32,               # Unsure if useable in this model, but whatever\n",
    "                    #verbose=0,                     # This will silence printing output\n",
    "                    validation_data=(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train) # Note: Multi-classification specific prep\n",
    "y_test  = to_categorical(y_test)  # Note: This one-hot encodes the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual model itself\n",
    "model = Sequential()\n",
    "model.add(Dense(4,                                # Start == to # of features, but not over 100\n",
    "               activation='relu',                 # Default.  Rarely Change\n",
    "               input_dim=X_train_scaled.shape[1]))\n",
    "model.add(Dense(num_of_classes_we_are_predicting, # This is the Output Layer\n",
    "                activation='softmax'))            # Activation is specific to the output layer\n",
    "model.compile(optimizer='adam',                   # Default.  Rarely change.\n",
    "#             metrics=['accuracy'],               # This is optional, if we wish to track it\n",
    "             loss = 'categorical_crossentropy')   # Default for this type?\n",
    "results = model.fit(X_train_scaled, y_train,\n",
    "                    validation_data = (X_test_scaled, y_test),\n",
    "                    #verbose=0,                   # This will silence printing output\n",
    "                    epochs = 10)                  # One of the first things to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisations for classification Models\n",
    "# Plots to view Loss/Accuracy over epoche's\n",
    "plt.plot(results.history['val_loss'], label='Test Loss')\n",
    "plt.plot(results.history['loss'], label='Train Loss')\n",
    "plt.legend();\n",
    "plt.plot(results.history['val_acc'], label='Test Accuracy')\n",
    "plt.plot(results.history['acc'], label='Train Accuracy')\n",
    "plt.legend();\n",
    "\n",
    "#Alternative\n",
    "fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15,4))\n",
    "\n",
    "ax[0].plot(results.history['loss'])\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[0].set_xlabel(\"epochs\")\n",
    "\n",
    "ax[1].plot(results.history['acc'])\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[1].set_xlabel(\"epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Property\t      |Use|\n",
    "|-----------------|---|\n",
    "|.param_grid\t  |parames searched over|\n",
    "|.best_score_\t  |Best mean cross-validated score|\n",
    "|.best_estimator_ |Model with best score. (callable)|\n",
    "|.best_params_\t  |The params found to perform w/best score|\n",
    "|.grid_scores_    |score attributes w/parameters|\n",
    "\n",
    "Grid Search Reg Penalties w/Log Reg\n",
    "\n",
    "|Argument|Description|\n",
    "|--------|--------|\n",
    "|penalty|'l1' for Lasso, 'l2' for Ridge|\n",
    "|solver\t|Must set to 'liblinear' for Lasso penalty to work.|\n",
    "|C\t    |Regularization strength. Equivalent to 1./alpha|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\n",
    "    'n_neighbors':[1,3,5,9,15,21],\n",
    "    'weights':['uniform','distance'],\n",
    "    'metric':['euclidean','manhattan']\n",
    "}\n",
    "knn_gridsearch = GridSearchCV(KNeighborsClassifier(), knn_params, cv=5, verbose=1, n_jobs=2)\n",
    "\n",
    "knn_gridsearch = knn_gridsearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_randomsearch = RandomizedSearchCV(KNeighborsClassifier(), knn_params, cv=5, \n",
    "                                      n_iter=10, verbose=1, n_jobs=2, random_state=42)\n",
    "\n",
    "knn_randomsearch = knn_randomsearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Regularization penalties\n",
    "gs_params = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'solver':['liblinear'],\n",
    "    'C':np.logspace(-5,0,100)\n",
    "}\n",
    "\n",
    "lr_gridsearch = GridSearchCV(LogisticRegression(), gs_params, cv=5, verbose=1)\n",
    "lr_gridsearch = lr_gridsearch.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "kmean = KMeans(n_clusters=3)\n",
    "kmean.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF out of the Centroids\n",
    "centroids_df = pd.DataFrame(kmean.cluster_centers_, columns= [\"Pu239\",'Pu240'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space Reserved for Function to plot in scatter form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Score for Clusters\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silho_score = silhouette_score(Xs_df, kms.labels_)\n",
    "silho_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "def ALL_METRICS(y_real, y_hat, p = X.shape[1]):\n",
    "    print('Mean Squared Error      :',metrics.mean_squared_error(y_real, y_hat))\n",
    "    print('Root Mean Squared Error :',np.sqrt(metrics.mean_squared_error(y_real, y_hat)))\n",
    "    print('Mean Squared Log Error  :',metrics.mean_squared_log_error(y_real, y_hat))\n",
    "    print('Median Absolute Error   :',metrics.median_absolute_error(y_real,y_hat))\n",
    "    print('R Squared               :',metrics.r2_score(y_real, y_hat))\n",
    "    print('Adjusted R Squared      :',r2_adj(y_real, y_hat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crontab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "time.time() # The time now, measured in seconds\n",
    "t0 = time.time()\n",
    "\n",
    "# Code goes here.\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stating the current datetime in a format as I understand\n",
    "\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "datetime.fromtimestamp(time()).strftime('%m-%d-%Y, %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big O Notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine('postgres://'+  # Something\n",
    "                       'u6jk064rvr8dao:'+ #Username\n",
    "                       'p0f453685110bbc3b5c17c10350750f46b60666c388a78134b4c5dc792da985f7'+ # Password\n",
    "                       '@ec2-34-202-213-35.compute-1.amazonaws.com:5432/'+ # The connection to it\n",
    "                       'dc5rooirs71hh0')  # ssssomething else\n",
    "   # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
